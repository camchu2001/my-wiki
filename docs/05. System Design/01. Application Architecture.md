> **Resource**
> - [Application Architecture](https://neetcode.io/courses/system-design-for-beginners/1)
### 1. A developer's perspective
![dev-perspective](https://i.imgur.com/Rc0XPBQ.png)
* Developers write code that is **deployed** to a **server**. For now, let's define a **server** as a computer that handles requests from another computer. 
* This server also requires **persistent storage** to store the application's data. As such, a server may talk to an external storage system (database, cloud etc). This storage may not be part of the same server, and is instead connected through a **network**.
### 2. A user’s perspective
![](https://i.imgur.com/SMJP3BH.png)
A user is someone who **makes a request to the server,** usually through a web browser. 
* If a user wanted to use a front-end feature, the server will respond with the JavaScript/HTML/CSS code, compiled to display what the user requested. 
* A user can also make a request to a back-end server API, the server will respond with data, possibly in the JSON format, or other formats. 
### 3. Scaling our server
> But, what if we have **a lot of users** and a **single server cannot handle** all of the requests on its own?

<u>**Solution 1:**</u> We can determine the bottleneck, maybe the CPU is not fast enough, maybe we don’t have enough RAM. 
→ we can upgrade the hardware of our single server to help it perform better. → **vertical scaling** (to take a single resource and make it better.)

<u>**Solution 2:**</u>  Because we cannot scale a single server infinitely, we can use **horizontal scaling**, which is to have multiple servers running our code.
→ users don’t have to talk to a single server but multiple → we can handle more requests at the same time → **better performance.**
→ if one server were to go down, we can direct traffic to other servers → **fault tolerant.**

For **simple applications, vertical scaling** may be sufficient and easier. For **large systems**, we prefer **horizontal scaling** because it’s much more powerful, and can be achieved with relatively inexpensive, standard hardware. However, it also requires much more engineering effort, as we need to ensure that the servers are communicating with each other, and the user requests are being distributed evenly. 
* When we have multiple servers handling request, we should have a **load balancer** (*a device or software program that distributes incoming traffic evenly across a group of multiple servers*) to forward the request to the server with the minimal amount of traffic. 

It's also important to remember that servers don't exist in isolation. It is highly likely that **servers are interacting with external servers, through APIs**. For example, an e-commerce website server might use a payment gateway API to process customer credit card transaction (interacting with the payment processor’s server.) 